{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Forest Classification in Mozambique\n",
    "\n",
    "This notebook presents a workflow for classifying forest cover in Mozambique using remote sensing data. The classification utilizes annual mosaics derived from Sentinel-2 and Landsat sources, with data organized by value quantiles for analysis.\n",
    "\n",
    "## Workflow Setup\n",
    "\n",
    "We begin by importing the necessary libraries for geospatial analysis, data handling, visualization, and machine learning. These tools form the foundation for the classification workflow that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import geopandas as gpd\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray  # noqa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from rasterio import Affine\n",
    "from rasterio.features import rasterize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "The next step is to load the remote sensing image data and the GeoJSON file containing the regions of interest (ROIs) for classification. These ROIs are defined as polygons that represent areas labeled as forest or non-forest. The polygon labels are created using QGIS and provide the ground truth required for supervised classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Paths to the data\n",
    "assets: Path = Path(\"assets\").resolve(strict=True)\n",
    "year: int = 2024\n",
    "filepath: Path = next(assets.glob(f\"*{year}*.nc\"))\n",
    "\n",
    "# Load Data\n",
    "feature_cube = xr.open_dataset(filepath)\n",
    "feature_cube = feature_cube.sel(quantile=slice(0.1, 0.9))  # remove outliers\n",
    "slivers: gpd.GeoDataFrame = gpd.read_file(assets / \"label_features_i02.geojson\")\n",
    "\n",
    "# Prepare the crs and affine transformation\n",
    "spatial_ref: dict[str, Any] = feature_cube[\"spatial_ref\"].attrs\n",
    "crs = spatial_ref[\"crs_wkt\"]\n",
    "slivers.to_crs(crs, inplace=True)\n",
    "geotransfrom: list[float] = [float(x) for x in spatial_ref[\"GeoTransform\"].split()]\n",
    "affine = Affine.from_gdal(*geotransfrom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the crs and affine transformation\n",
    "spatial_ref: dict[str, Any] = feature_cube[\"spatial_ref\"].attrs\n",
    "crs = spatial_ref[\"crs_wkt\"]\n",
    "slivers.to_crs(crs, inplace=True)\n",
    "_geotransfrom: list[float] = [float(x) for x in spatial_ref[\"GeoTransform\"].split()]\n",
    "affine = Affine.from_gdal(*_geotransfrom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Defining land cover types as an enumeration provides a clear mapping between each class and its numeric value. This improves code readability and ensures consistency when assigning and interpreting classification labels throughout the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the labels a meaningful name\n",
    "\n",
    "\n",
    "class LandCoverType(Enum):\n",
    "    \"\"\"Binding land cover types to their numeric values.\"\"\"\n",
    "\n",
    "    TREE_COVER = 10\n",
    "    SHRUBLAND = 20\n",
    "    GRASSLAND = 30\n",
    "    CROPLAND = 40\n",
    "    SETTLEMENT = 50\n",
    "    BARE_SPARSE = 60\n",
    "    SNOW_ICE_ABSENT = 70\n",
    "    PERMANENT_WATER = 80\n",
    "    INTERMITTENT_WATER = 86\n",
    "    WETLAND = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "We will now plot the image data alongside the regions of interest (ROIs) to obtain a comprehensive understanding of the spatial distribution of the labeled areas. This visualization will help us verify the alignment of the labeled polygons with the underlying remote sensing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RGB Image\n",
    "fig, ax = plt.subplots()\n",
    "feature_cube[[\"Red\", \"Green\", \"Blue\"]].sel(quantile=0.5).to_dataarray().plot.imshow(\n",
    "    robust=True,\n",
    "    ax=ax,\n",
    ")\n",
    "slivers.plot(ax=ax, column=\"label\", cmap=\"RdYlBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Prepare Data for Classification\n",
    "\n",
    "Prior to the actual machine learning classification, the data needs to be preprocessed and organized into a suitable format. The preparation involves several key steps:\n",
    "1. Rasterization of the polygons to create a mask for the areas of interest.\n",
    "2. Extraction of pixel values from the image data using the mask.\n",
    "3. Creation of a feature matrix and target vector for classification from the image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://rasterio.readthedocs.io/en/stable/api/rasterio.features.html#rasterio.features.rasterize\n",
    "# Rasterize the polygons\n",
    "rst = rasterize(\n",
    "    shapes=slivers.to_crs(crs)[[\"geometry\", \"poly_id\"]]\n",
    "    .to_records(index=False)\n",
    "    .tolist(),\n",
    "    out_shape=feature_cube[\"Red\"].shape[1:],  # only x and y dimensions\n",
    "    transform=affine,\n",
    ")\n",
    "\n",
    "# Create a DataArray from the rasterized polygons\n",
    "label_cube = xr.DataArray(\n",
    "    rst,\n",
    "    coords={\"y\": feature_cube.y, \"x\": feature_cube.x},\n",
    "    name=\"poly_id\",\n",
    ")\n",
    "\n",
    "# Create a DataFrame from the image data\n",
    "feature_frame = (\n",
    "    feature_cube.drop_vars(\"spatial_ref\")\n",
    "    .to_dataframe()\n",
    "    .dropna()\n",
    "    .unstack(level=\"quantile\")\n",
    ")\n",
    "\n",
    "# Rename the columns to include quantile information\n",
    "feature_frame.columns = [\n",
    "    f\"{c[0]}_P{np.round(100 * c[1], 0).astype(int):03}\" for c in feature_frame.columns\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the label data (originally the polygons)\n",
    "label_frame = (\n",
    "    label_cube.to_dataframe()\n",
    "    .reset_index()\n",
    "    .merge(slivers[[\"poly_id\", \"label\"]])\n",
    "    .set_index([\"y\", \"x\"])\n",
    ")\n",
    "train_frame = label_frame.join(feature_frame)\n",
    "\n",
    "# Prepare the features and target variable for classification\n",
    "X_features = train_frame.drop(columns=[\"label\", \"poly_id\"])\n",
    "y_target = train_frame[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Classify Forest Cover with Scikit-learn\n",
    "\n",
    "We will proceed with training a Random Forest classifier using the prepared data.\n",
    "This process involves the following key steps:\n",
    "1. Data Splitting into Training and Testing Sets to evaluate the model's performance.\n",
    "2. Model Training on the Training Set.\n",
    "3. Model Evaluation on the Testing Set.\n",
    "4. Visualization of Results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training and testing datasets\n",
    "SEED = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_features,\n",
    "    y_target,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "# Classify Forest Cover with Scikit-learn\n",
    "randforest = RandomForestClassifier(\n",
    "    random_state=SEED,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "randforest_test = randforest.fit(X_train, y_train)\n",
    "randforest_predict = randforest.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "With the trained model, we can now make predictions on the image data.\n",
    "The Random Forest classifier will be used to predict the forest cover for each pixel in the image.\n",
    "For that the image data needs to be prepared in a similar way as the training data, as the model expects essentially a vector of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the image data for classification\n",
    "img = (\n",
    "    feature_cube.drop_vars(\"spatial_ref\")\n",
    "    .to_dataarray()\n",
    "    .transpose(\"x\", \"y\", \"quantile\", \"variable\")\n",
    ")\n",
    "\n",
    "# Reshape the image data\n",
    "num_of_pixels = img.sizes[\"x\"] * img.sizes[\"y\"]\n",
    "num_of_bands = img.sizes[\"quantile\"] * img.sizes[\"variable\"]\n",
    "X_image_data = img.values.reshape(num_of_pixels, num_of_bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the trained model\n",
    "randforest_predict_img = randforest.predict(X_image_data)\n",
    "randforest_predict_img = randforest_predict_img.reshape(\n",
    "    img.sizes[\"x\"],\n",
    "    img.sizes[\"y\"],\n",
    ").transpose()\n",
    "\n",
    "# Recreate the DataArray for the predicted forest cover\n",
    "predicted_forest = (\n",
    "    xr.DataArray(\n",
    "        randforest_predict_img,\n",
    "        dims=(\"y\", \"x\"),\n",
    "        coords={\n",
    "            \"x\": feature_cube[\"x\"],\n",
    "            \"y\": feature_cube[\"y\"],\n",
    "        },\n",
    "    )\n",
    "    .rio.write_crs(crs)\n",
    "    .rio.write_transform(affine)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the predicted forest classification\n",
    "predicted_forest.where(predicted_forest == LandCoverType.TREE_COVER.value).hvplot.image(\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    tiles=True,\n",
    "    cmap=\"greens\",\n",
    "    crs=crs,\n",
    "    hover=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the classification report\n",
    "print(classification_report(y_test, randforest_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the results for future analysis\n",
    "savepath = Path(f\"forest_classification_{year}.tif\")\n",
    "predicted_forest.rio.to_raster(savepath, driver=\"GTiff\", compress=\"lzw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environmental-remote-sensing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
