{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Mosaics\n",
    "\n",
    "Even though the processing algorithms for optical data have been designed with great care, the images still can contain imperfections, such as unmasked clouds or shadows. And even if they are masked, we could be left with images that have massive gaps! A common solution to solve both the gaps and imperfections is the creation of multi-temporal mosaics. This term covers various techniques designed to “patch” areas covered by clouds/shadows whilst trying to avoid including abnormal values (called \"artifacts\"). On simple terms, is using several observation so the “holes” in one image are filled by another. A common technique to create annual mosaics is to calculate of annual quantiles. This specific statistic is chosen because it is just concerned about the order, ensuring artifacts are left in a very low (e.g., 0 or 0.05, shadows) or a very high quantile (e.g., 0.95, 1.0, clouds). However, statistics like the mean could be tainted by outliers as they are affected by all the observations. Mosaics are [Level-3 products](https://www.earthdata.nasa.gov/learn/earth-observation-data-basics/data-processing-levels), periodic (statistical) summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "\n",
    "from envrs.hls_tools import preprocess_bands, preprocess_fmask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Set the directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARENT_DIR = Path(r\"~/Downloads/hls\").expanduser()\n",
    "\n",
    "IN_DIR = PARENT_DIR / \"automated\"\n",
    "\n",
    "IN_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## List the available files\n",
    "\n",
    "First we look for the files containing the bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are looking for products containing...\n",
    "product, tile, res, version = \"HLS\", \"T36KXE\", \"30\", \"v2.0\"\n",
    "\n",
    "# And they also have to end with \"bands.tif\"\n",
    "band_paths = sorted(IN_DIR.glob(f\"{product}_{tile}_*{res}_{version}_bands.tif\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    " Then we take only the matching `fmask` files (remember, the bands were not downloaded if the image was too cloudy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmask_paths = [p.parent / p.name.replace(\"_bands\", \"_fmask\") for p in band_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Stack the individual files into a data cube\n",
    "\n",
    "the [`Xarray`](https://tutorial.xarray.dev/intro.html) library allows us to open many files together as a single [datacube](https://en.wikipedia.org/wiki/Data_cube), which can be very convenient for handling large volumes of data. To make our mosaics we will have to read both the bands, but also the `fmask` files, as we need to exclude the cloudy/shaded observations.\n",
    "\n",
    "We will use [`xarray.open_mfdataset`](https://docs.xarray.dev/en/stable/generated/xarray.open_mfdataset.html) to read the files and concatenate them along the `time` dimension. The argument `preprocess` allows to transform the input data as needed to ensure the datacube creation is as smooth as possible. The tool is run twice, one for `band` and another for `time`, because their preprocessing is different. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Preprocess the Fmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmask = xr.open_mfdataset(\n",
    "    fmask_paths,\n",
    "    chunks=\"auto\",\n",
    "    concat_dim=\"time\",\n",
    "    combine=\"nested\",\n",
    "    preprocess=preprocess_fmask,\n",
    "    mask_and_scale=False,\n",
    "    engine=\"rasterio\",\n",
    "    parallel=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Preprocess the bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = xr.open_mfdataset(\n",
    "    band_paths,\n",
    "    chunks=\"auto\",\n",
    "    concat_dim=\"time\",\n",
    "    combine=\"nested\",\n",
    "    preprocess=preprocess_bands,\n",
    "    engine=\"rasterio\",\n",
    "    parallel=True,\n",
    "    band_as_variable=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Make a raster definition\n",
    "\n",
    "Our mosaics will need to \"inherit\" the same CRS and geotransform as the original files, and an `encoding` that ensures the image file volume is kept to a minimum. We will we will get these by using the first file as an example, and place this information on dictionary to use it during the writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_bands = rxr.open_rasterio(band_paths[0])  # .encoding\n",
    "\n",
    "crs = example_bands.rio.crs\n",
    "transform = example_bands.rio.transform()\n",
    "\n",
    "example_attrs = example_bands.attrs\n",
    "example_encoding = example_bands.encoding\n",
    "\n",
    "out_attrs = {}\n",
    "out_encoding = {\n",
    "    \"dtype\": example_encoding[\"rasterio_dtype\"],\n",
    "    \"add_offset\": example_attrs[\"add_offset\"],\n",
    "    \"scale_factor\": example_attrs[\"scale_factor\"],\n",
    "    \"_FillValue\": example_attrs[\"_FillValue\"],\n",
    "    \"zlib\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Mask and make the yearly mosaics\n",
    "\n",
    "Once we have everything ready we select the data from a `target_year`. If `any` `cloud_flag` in the `flag` dimension was `True` the observation will be ignored. The rest of the data is used to calculate the quantiles along the time dimension (`0.0` - `1.0`, with a `0.1` step). The last steps are setting the `CRS`, the (geo)`transform`, the attributes and the encoding just before writing `to_netcdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which years should be processed\n",
    "first_year = bands[\"time\"].min().dt.year.item()\n",
    "last_year = bands[\"time\"].max().dt.year.item()\n",
    "\n",
    "# What to exclude\n",
    "cloud_flags = [\"cloud shadow\", \"adjacent to cloud\", \"cloud\", \"cirrus cloud\"]\n",
    "\n",
    "# For every year\n",
    "for target_year in range(first_year, last_year + 1):\n",
    "    out_path = PARENT_DIR / f\"{product}_{tile}_{target_year}_b{res}_{version}.nc\"\n",
    "    if out_path.exists():\n",
    "        continue\n",
    "\n",
    "    # define what to mask\n",
    "    is_cloudy = (\n",
    "        fmask[\"masks\"]\n",
    "        .sel(time=(fmask.time.dt.year == target_year), flag=cloud_flags)\n",
    "        .any(dim=\"flag\")\n",
    "    )\n",
    "\n",
    "    # Mask the bands, calculate the quantiles\n",
    "    quantiles = (\n",
    "        bands.sel(time=(bands.time.dt.year == target_year))\n",
    "        .sortby(\"time\")\n",
    "        .where(np.logical_not(is_cloudy))\n",
    "        .quantile(np.arange(0, 1.01, 0.1), dim=\"time\", skipna=True)\n",
    "        .sortby(\"y\", ascending=False)\n",
    "    )\n",
    "\n",
    "    # Apply the scaling\n",
    "    referenced = quantiles.rio.write_crs(crs).rio.write_transform(transform)\n",
    "\n",
    "    # set the attributes and the encoding\n",
    "    for band_name in quantiles:\n",
    "        referenced[band_name].attrs.update(long_name=band_name, **out_attrs)\n",
    "        referenced[band_name].encoding.update(**out_encoding)\n",
    "\n",
    "    # Write\n",
    "    referenced.to_netcdf(out_path)\n",
    "\n",
    "    print(target_year)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environmental-remote-sensing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
