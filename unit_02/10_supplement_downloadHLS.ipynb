{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Read relevant portions of the HLS dataset from NASA earthdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import earthaccess as ea\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "\n",
    "from envrs import hls_tools as hls\n",
    "from envrs import rio_tools as rt\n",
    "from envrs.download_path import make_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "To be able to download the data you will need to have an account on NASA Earthdata. You can register using the following link:\n",
    "\n",
    "https://urs.earthdata.nasa.gov/\n",
    "\n",
    "<img src=\"../images/portal_eartdata_login.png\" width=\"700\" alt=\"Earthdata login\">\n",
    "\n",
    "To be able to log you in, the next cell will ask you for the user and password and that you set during the registration. These are used to create the `~/.netrc` file that is used by the library to grant access to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the netrc path\n",
    "netrc_path = Path(\"~/.netrc\").expanduser()\n",
    "\n",
    "\n",
    "# Ensure .netrc is written where it should\n",
    "def write_netrc(auth, netrc_path=netrc_path):\n",
    "    username = auth.username\n",
    "    password = auth.password\n",
    "    text = f\"machine urs.earthdata.nasa.gov\\n\\tlogin {username}\\n\\tpassword {password}\"\n",
    "    return netrc_path.write_text(text)\n",
    "\n",
    "\n",
    "# are we authenticated?\n",
    "auth = ea.login()\n",
    "\n",
    "if not auth.authenticated:\n",
    "    # ask for credentials and persist them in a .netrc file\n",
    "    auth.login(strategy=\"interactive\", persist=True)\n",
    "\n",
    "# Write the ~/.netrc file if it does not exist\n",
    "if not netrc_path.exists():\n",
    "    write_netrc(auth=auth, netrc_path=netrc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Identify the target datasets\n",
    "\n",
    "The `earthaccess` package allows to check which datasets are available on the [NASA earthdata repository](https://search.earthdata.nasa.gov/search) and also download them. Here we are looking for datasets whose name starts with `HLS`, and acronym standing for \"Harmonized Landsat/Sentinel-2\". Both datasets and images are described using an standard called [Unified Metadata Model](https://www.earthdata.nasa.gov/fr/about/esdis/esco/standards-practices/unified-metadata-model) (\"UMM\"). That's why the `\"umm\"` keyword will be used many times along the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tree = ea.search_datasets(keyword=\"HLS*30\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "The result is a [`list`](https://realpython.com/python-data-structures/#list-mutable-dynamic-arrays) of [`dictionaries`](https://realpython.com/python-data-structures/#list-mutable-dynamic-arrays), where each contains the details of one dataset. In this case we are just displaying the `\"ShortName\"` (within the `umm` portion) to see which datasets matched with the `\"HLS\"` keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "[d[\"umm\"][\"ShortName\"] for d in dataset_tree]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "`HLSS30` is the dataset here images Have originated from Sentinel-2, and `HLSL30` is the equivalent when images have originated from Landsat 8/9. These are kept separate because not all the spectal bands are present in the other, and also the numbering/naming varies across sensors. Unintendedly we found a couple of additional datasets ending in `*_VI`, which are are vegetation indices derived from the `HLSS20` and `HLSL30`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Query NASA datasets to identify overlapping datasets\n",
    "\n",
    "We will use the `HLSextent.geojson` file to select the images covering our study area. For this, we read the file, take the geometry of the first row (in this case, the only one), and calculate the enclosing rectangle containing the polygon (bounding box, shortened to `bbox`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi_uri = make_url(\"HLSextent.geojson\")\n",
    "\n",
    "aoi = gpd.read_file(aoi_uri)\n",
    "geometry = aoi.loc[0, \"geometry\"]\n",
    "\n",
    "bbox = geometry.bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Then we use `ea.search_data` to see which datasets overlap the area depicted in the spatial file (`bounding_box`) and where acquired during our period of interest (`temporal`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "granule_pile = ea.search_data(\n",
    "    short_name=[\"HLSL30\", \"HLSS30\"],\n",
    "    cloud_hosted=True,\n",
    "    temporal=(\"2018-01-01T00:00:00\", \"2024-12-31T23:59:59\"),\n",
    "    bounding_box=bbox,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Examine the contents of a granule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "The result of our search is stored in `granule_pile`, a Python `list` where each entry is an object detailing the information of the image (also called `granule`) and all the files available for said image. Some of these are bands (`B*` suffix), whereas `Fmask` is the cloud mask, and the ones ending on `*A` are sun or observation angles. Here on a notebooks these objects open a display where we can see a small color composite of the image, as well as some buttons to download the files. However, most of the time is more comfortable to do it on a more automated way (keep reading)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_granule = granule_pile[0]\n",
    "first_granule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "The next cell shows how we can access the information describing the granule. Once again the `\"umm\"` holds the relevant data we may need such as the [MGRS tile](https://hls.gsfc.nasa.gov/products-description/tiling-system/) of the files (`\"MGRS_TILE_ID\"`) or their cloud cover (`\"CLOUD_COVERAGE\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(first_granule.items())[\"umm\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Retain only one of the tiles, when the cloud cover is <90%\n",
    "\n",
    "We will use the information about the granules/images to filter the images to download. We prepared the function `hls.extract_extra_attrs` to help us to extract the relevant information. Now that we have it we keep just the images corresponding to the [MGRS tile](https://hls.gsfc.nasa.gov/products-description/tiling-system/) `\"T36KXE\"` (Sentinel-2 tiles have some overlap). We also exclude images/granules where `\"CLOUD_COVERAGE\"` is greater than 90% because we would not see anything on the ground."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_valid = 10.0\n",
    "max_cloud = 100 - min_valid\n",
    "target_tile = \"T36KXE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_pile, name_pile = [], []\n",
    "for granule in granule_pile:\n",
    "    # Extract the relevant attributes\n",
    "    granule_attrs = hls.extract_extra_attrs(granule)\n",
    "\n",
    "    # Skip if the target tile does not match\n",
    "    if granule_attrs[\"MGRS_TILE_ID\"] not in target_tile:\n",
    "        continue\n",
    "\n",
    "    # Skip if the cloud cover is too high\n",
    "    if float(granule_attrs[\"CLOUD_COVERAGE\"]) > max_cloud:\n",
    "        continue\n",
    "\n",
    "    # ad the URI's of the selected rasters to the pile\n",
    "    layer_pile.extend(granule.data_links())\n",
    "    name_pile.append(granule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Format the dataframe of the downloads\n",
    "\n",
    "Again we use a couple of helper functions to clean up the result of our search. `hls.tabulate_hls_uris` formats the results as a table ([dataframe](https://pandas.pydata.org/docs/user_guide/dsintro.html#dataframe)), where the column `uri` has the link to every dataset available for the image granule, whereas the rest of columns are just information contained on the dataset name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_frame = hls.tabulate_hls_uris(layer_pile)\n",
    "sel_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "It is important to note that the band numbers are [NOT shared](https://www.earthdata.nasa.gov/data/projects/hls/spectral-bands) by the `L30` and the `S30` products. Furthermore, the numbers do not necessarily point to the same spectral band, and some bands are entirely absent in one product or another (`L30` does not have thermal, `S30` does not have red edge bands)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_frame.groupby([\"sensor\", \"suffix\"]).size().unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "To make things easier we have prepared the `hls.harmonize_hls_frame`, that reformats our table so every column is named after a specific portion of the spectrum. These columns contain the links to the files from the `L30` and `S30` depicting the same portion. That way we can download just the bands shared by both sensors in a format that is easy to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "harmonized_frame = hls.harmonize_hls_frame(sel_frame)\n",
    "harmonized_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Now every row contains `stem`, the base name we will use for our output files, and the rest of the columns containing the links to the datasets in the granule.\n",
    "\n",
    "* `CoastalAerosol`, `Blue`, `Green`, `Red`, `NIRnarrow`, `SWIR1`, `SWIR2` are the bands\n",
    "* `Fmask` contains information to mak the presence of clouds, shadows, water or snow\n",
    "* The rest are the sun azimuth and zenith angles (`SAA`, `SZA`) and the view zenith and azimuth angles (`VAA`, `VZA`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Drop the coastal aerosol and the angle bands: they will not be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_frame = harmonized_frame.drop(\n",
    "    columns=[\"CoastalAerosol\", \"SZA\", \"SAA\", \"VZA\", \"VAA\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Perform download with conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Set the environment for the download:\n",
    "\n",
    "This cell sets a series of [environment variables](https://en.wikipedia.org/wiki/Environment_variable) necessary to be able to read the image data over the internet using the [geospatial data abstraction library (GDAL)](https://courses.spatialthoughts.com/gdal-tools.html#introduction), an extremely powerful piece of software for working with spatial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/nasa/HLS-Data-Resources/blob/main/python/tutorials/HLS_Tutorial.ipynb\n",
    "env_pairs = {\n",
    "    \"GDAL_HTTP_COOKIEFILE\": \"~/cookies.txt\",\n",
    "    \"GDAL_HTTP_COOKIEJAR\": \"~/cookies.txt\",\n",
    "    \"GDAL_DISABLE_READDIR_ON_OPEN\": \"EMPTY_DIR\",\n",
    "    \"CPL_VSIL_CURL_ALLOWED_EXTENSIONS\": \"TIF\",\n",
    "    \"GDAL_HTTP_UNSAFESSL\": \"YES\",\n",
    "    \"GDAL_HTTP_MAX_RETRY\": \"10\",\n",
    "    \"GDAL_HTTP_RETRY_DELAY\": \"0.5\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Set the output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = Path(r\"~/Downloads/hls/automated\").expanduser()\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### Select the bands we want to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\"Blue\", \"Green\", \"Red\", \"NIRnarrow\", \"SWIR1\", \"SWIR2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### And download (but!)\n",
    "\n",
    "In this cell is where the magic happens. We read a portion of the `fmask` dataset over the internet, and if too many pixels are masked as cloudy, we do not download anything else. If the image is clear enough, we also download the relevant portion from the bands. The cell also skip the files that already have been downloaded, and only reads the portion of our images covered by our AOI. The idea is to reduce data volume and the wait time required to acquire the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "with rio.Env(**env_pairs) as download_env:\n",
    "    for row_idx, (product_prefix, product_uris) in enumerate(reduced_frame.iterrows()):\n",
    "        # Print progress\n",
    "        if row_idx % 100 == 0:\n",
    "            print(f\"{row_idx:>04}\", \"/\", reduced_frame.shape[0], product_prefix)\n",
    "\n",
    "        # check if the fmask file exists, skip if that's the case\n",
    "        fmask_path = OUT_DIR / f\"{product_prefix}_fmask.tif\"\n",
    "        if fmask_path.is_file():\n",
    "            continue\n",
    "\n",
    "        # If does not exist, download and save\n",
    "        fmask_pairs, fmask_profiles, fmask_tags = rt.clipped_read(\n",
    "            product_uris[[\"Fmask\"]], aoi\n",
    "        )\n",
    "        rt.write_raster(fmask_pairs, fmask_profiles, fmask_tags, fmask_path)\n",
    "\n",
    "        # Decompose the bit flags\n",
    "        fmask_number = next(iter(fmask_pairs.values()))\n",
    "        fmask_flags = np.flip(\n",
    "            np.unpackbits(np.expand_dims(fmask_number, axis=0), axis=0), axis=0\n",
    "        )\n",
    "\n",
    "        # Estimate the percentage of clear pixels\n",
    "        # (not cirrus, cloud core/adjacent or shadow casted by them)\n",
    "        is_clear = np.all(fmask_flags[0:4] == 0, axis=0)\n",
    "        perc_valid = 100 * np.sum(is_clear) / fmask_number.size\n",
    "\n",
    "        # Skip band download if the percentage of valid pixels is under threshold\n",
    "        if perc_valid < min_valid:\n",
    "            continue\n",
    "\n",
    "        # check if the band file exists, skip if that's the case\n",
    "        band_path = OUT_DIR / f\"{product_prefix}_bands.tif\"\n",
    "        if band_path.is_file():\n",
    "            continue\n",
    "\n",
    "        # read the bands\n",
    "        band_pairs, band_profiles, band_tags = rt.clipped_read(\n",
    "            product_uris[selected_columns], aoi\n",
    "        )\n",
    "\n",
    "        # Write to the hard drive\n",
    "        rt.write_raster(band_pairs, band_profiles, band_tags, band_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environmental-remote-sensing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
