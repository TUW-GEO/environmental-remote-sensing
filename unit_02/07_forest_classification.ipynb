{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Forest Classification in Mozambique\n",
    "\n",
    "This notebook presents a workflow for classifying forest cover in Mozambique using remote sensing data. The classification utilizes annual mosaics derived from Sentinel-2 and Landsat sources, with data organized by value quantiles for analysis.\n",
    "\n",
    "![](https://i.natgeofe.com/n/695375d1-08d7-424a-b800-e56073faeec9/gorongosa-national-wildlife-park-3.jpg?w=1440&h=960)\n",
    "_National Geographic - https://www.nationalgeographic.com/magazine/article/mozambique-gorongosa-national-park-wildlife-rebound_\n",
    "\n",
    "## Workflow Setup\n",
    "\n",
    "We begin by importing the necessary libraries for geospatial analysis, data handling, visualization, and machine learning. These tools form the foundation for the classification workflow that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import geopandas as gpd\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray  # noqa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from rasterio import Affine\n",
    "from rasterio.features import rasterize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/download_path.py\n",
    "\n",
    "year: int = 2018  # change to 2018, 2020 or 2024\n",
    "url_dc = make_url(\n",
    "    f\"HLS_T36KXE_{year}_b30_v2.zarr.zip\",\n",
    "    branch=\"dev\",\n",
    "    is_zip=True,\n",
    ")\n",
    "url_slivers = make_url(\n",
    "    \"label_features_i02.geojson\",\n",
    "    branch=\"dev\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "The next step is to load the remote sensing image data and the GeoJSON file containing the regions of interest (ROIs) for classification. These ROIs are defined as polygons that represent areas labeled as forest or non-forest. The polygon labels are created using QGIS and provide the ground truth required for supervised classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "feature_cube = xr.open_dataset(url_dc, engine=\"zarr\", consolidated=False)\n",
    "feature_cube = feature_cube.sel(quantile=slice(0.1, 0.9))  # remove outliers\n",
    "slivers: gpd.GeoDataFrame = gpd.read_file(url_slivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the crs and affine transformation\n",
    "spatial_ref: dict[str, Any] = feature_cube[\"spatial_ref\"].attrs\n",
    "crs: str = spatial_ref[\"crs_wkt\"]\n",
    "_geotransfrom: list[float] = [float(x) for x in spatial_ref[\"GeoTransform\"].split()]\n",
    "affine = Affine.from_gdal(*_geotransfrom)\n",
    "\n",
    "# Set the crs on the polygons\n",
    "slivers.to_crs(crs, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Defining land cover types as an enumeration provides a clear mapping between each class and its numeric value. This improves code readability and ensures consistency when assigning and interpreting classification labels throughout the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandCoverType(Enum):\n",
    "    \"\"\"Binding land cover types to their numeric values.\"\"\"\n",
    "\n",
    "    TREE_COVER = 10\n",
    "    SHRUBLAND = 20\n",
    "    GRASSLAND = 30\n",
    "    CROPLAND = 40\n",
    "    SETTLEMENT = 50\n",
    "    BARE_SPARSE = 60\n",
    "    SNOW_ICE_ABSENT = 70\n",
    "    PERMANENT_WATER = 80\n",
    "    INTERMITTENT_WATER = 86\n",
    "    WETLAND = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "We will now plot the image data alongside the regions of interest (ROIs) to obtain a comprehensive understanding of the spatial distribution of the labeled areas. This visualization will help us verify the alignment of the labeled polygons with the underlying remote sensing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RGB Image\n",
    "fig, ax = plt.subplots()\n",
    "feature_cube[[\"Red\", \"Green\", \"Blue\"]].sel(quantile=0.5).to_dataarray().plot.imshow(\n",
    "    robust=True,\n",
    "    ax=ax,\n",
    ")\n",
    "slivers.plot(ax=ax, column=\"label\", cmap=\"RdYlBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Prepare Data for Classification\n",
    "\n",
    "Prior to the actual machine learning classification, the data needs to be preprocessed and organized into a suitable format. The preparation involves several key steps:\n",
    "1. Rasterization of the polygons to create a mask for the areas of interest.\n",
    "2. Extraction of pixel values from the image data using the mask.\n",
    "3. Creation of a feature matrix and target vector for classification from the image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://rasterio.readthedocs.io/en/stable/api/rasterio.features.html#rasterio.features.rasterize\n",
    "# Rasterize the polygons\n",
    "rst: np.ndarray = rasterize(\n",
    "    shapes=slivers.to_crs(crs)[[\"geometry\", \"poly_id\"]]\n",
    "    .to_records(index=False)\n",
    "    .tolist(),\n",
    "    out_shape=feature_cube[\"Red\"].shape[1:],  # only x and y dimensions\n",
    "    transform=affine,\n",
    ")\n",
    "\n",
    "# Create a DataArray from the rasterized polygons\n",
    "label_cube = xr.DataArray(\n",
    "    rst,\n",
    "    coords={\"y\": feature_cube.y, \"x\": feature_cube.x},\n",
    "    name=\"poly_id\",\n",
    ")\n",
    "\n",
    "# Create a DataFrame from the image data\n",
    "feature_frame: pd.DataFrame = (\n",
    "    feature_cube.drop_vars(\"spatial_ref\")\n",
    "    .to_dataframe()\n",
    "    .dropna()\n",
    "    .pivot_table(index=[\"y\", \"x\"], columns=\"quantile\", values=None)\n",
    "    # .unstack(level=\"quantile\")\n",
    ")\n",
    "\n",
    "\n",
    "# Rename the columns to include quantile information\n",
    "def _rename_col(col: tuple[str, float]) -> str:\n",
    "    return f\"{col[0]}_P{np.round(100 * col[1], 0).astype(int):03}\"\n",
    "\n",
    "\n",
    "feature_frame.columns = [_rename_col(col) for col in feature_frame.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the label data (originally the polygons)\n",
    "label_frame: pd.DataFrame = (\n",
    "    label_cube.to_dataframe()\n",
    "    .reset_index()\n",
    "    .merge(slivers[[\"poly_id\", \"label\"]])\n",
    "    .set_index([\"y\", \"x\"])\n",
    ")\n",
    "SEED: int = 42\n",
    "train_frame: pd.DataFrame = (\n",
    "    label_frame.join(feature_frame)\n",
    "    .groupby(\"poly_id\")\n",
    "    .sample(frac=0.7, random_state=SEED)\n",
    ")\n",
    "\n",
    "# Prepare the features and target variable for classification\n",
    "X_train: pd.DataFrame = train_frame.drop(columns=[\"label\", \"poly_id\"])\n",
    "y_train: pd.Series = train_frame[\"label\"]\n",
    "\n",
    "_drop_idx: pd.Index = train_frame.index\n",
    "y_test: pd.Series = label_frame.join(feature_frame).drop(index=_drop_idx)[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Classify Forest Cover with Scikit-learn\n",
    "\n",
    "We will proceed with training a Random Forest classifier using the prepared data.\n",
    "This process involves the following key steps:\n",
    "1. Data Splitting into Training and Testing Sets to evaluate the model's performance.\n",
    "2. Model Training on the Training Set.\n",
    "3. Model Evaluation on the Testing Set.\n",
    "4. Visualization of Results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify Forest Cover with Scikit-learn\n",
    "clf_rf = RandomForestClassifier(\n",
    "    random_state=SEED,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "clf_rf.fit(X=X_train, y=(y_train == 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "With the trained model, we can now make predictions on the image data.\n",
    "The Random Forest classifier will be used to predict the forest cover for each pixel in the image.\n",
    "For that the image data needs to be prepared in a similar way as the training data, as the model expects essentially a vector of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the trained model\n",
    "class_frame = pd.DataFrame(\n",
    "    clf_rf.predict(feature_frame),\n",
    "    index=feature_frame.index,\n",
    "    columns=[\"RF\"],\n",
    ")\n",
    "\n",
    "# Recreate the DataArray for the predicted forest cover\n",
    "predicted_forest = (\n",
    "    class_frame.to_xarray().rio.write_crs(crs).rio.write_transform(affine)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the predicted forest classification\n",
    "predicted_forest.where(predicted_forest == 1).hvplot.image(\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    tiles=True,\n",
    "    cmap=\"greens\",\n",
    "    crs=crs,\n",
    "    hover=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Now that we have completed the classification, we can have a look at the quality of the results.\n",
    "We can use various metrics to assess the accuracy of our classification. Here scikit-learn provides us with a convenient function to display a classification report.\n",
    "Finally after the classification is complete, we can save the results for future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the classification report\n",
    "print(classification_report(y_test, class_frame.loc[y_test.index, \"RF\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the results for future analysis\n",
    "savepath = Path(f\"forest_classification_{year}.zarr\")\n",
    "predicted_forest.astype(np.uint8).to_zarr(\n",
    "    savepath,\n",
    "    zarr_format=2,\n",
    "    consolidated=False,\n",
    "    mode=\"w\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environmental-remote-sensing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
