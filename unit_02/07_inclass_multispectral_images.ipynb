{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Multispectral data for deforestation detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## The electromagnetic spectrum\n",
    "\n",
    "What we perceive as light is just a small portion of the full electromagnetic spectrum. It is organized by wavelength (λ), which control the interaction with matter. Different sensors use different parts of the spectrum for specific applications: microwave remote sensing uses wavelengths on a centimeter scale to analyze soil water content, and thermal remote sensing estimates land surface temperature with wavelengths on the micrometer scale (10-6 m). Multi-spectral sensors register the energy in frequency bands whose wavelengths are described on a micrometer or a nanometer scale (10-9 m).\n",
    "\n",
    "The source of the energy received by the sensor depends on the wavelength it records. Microwave sensors can generate their own illumination (active; scatterometers, synthetic aperture radar), or register the energy emitted by the earth (passive; e.g., radiometers). Thermal and multi-spectral sensors are passive, with the former registering the amount of energy emitted by the earth, and the latter, which registers how much of the sun’s energy is reflected by the Earth.\n",
    "\n",
    "The most used regions on multi-spectral remote sensing are the visible region (0.4 – 0.7 μm) is the part of the spectrum our eyes can perceive. As wavelength increases we veer into the infrared, region, which usually is separated into near-infrared (NIR, 0.7 – 1.2 μm) and middle- or shortwave infrared (SWIR, 1.2 – 3 μm). Over the next section we will see what is the usefulness of these bands.\n",
    "\n",
    "<img src=\"../images/electromagnetic_spectrum.png\" width=\"700\"\n",
    "         alt=\"The electromagnetic spectrum\">\n",
    "\n",
    "_The electromagnetic spectrum (Modified from [Wikipedia commons](https://en.wikipedia.org/wiki/Electromagnetic_spectrum#/media/File:EM_Spectrum_Properties_edit.svg))_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## An intuitive view of multi-spectral remote sensing\n",
    "### The human eye as a sensor\n",
    "\n",
    "The human eye contains two types of light-sensitive the cells: rods and cones. The former are concerned with lightness and motion, whereas the latter enable color vision. Each type of cone is sensitive to a specific portion of the spectrum, with areas where are they have no sensitivity, areas with some, and a point where it is at its maximum (peak sensitivity). Our three types cone cells allow us to perceive red, green, blue and their mixtures. Other animals can see fewer (e.g., the dogs, 2) or more colors (e.g., birds such as the European starling, 4). Color sensitivity as a concept extends to the next closest example: the cameras.\n",
    "\n",
    "<img src=\"../images/eye_response.png\" alt=\"Retinal response\" width=\"700\">\n",
    "\n",
    "_Relative probability of absorption for the different rods of the human and the European Starling (bird) eye (From [Batchelor et al 2012](https://doi.org/10.1007/978-1-84996-169-1_3))_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Common Cameras\n",
    "\n",
    "Cameras use a lens to focus illumination reflected by the objects onto a sensor (digital) or film (analog). In both cases, a shutter controls how much energy enters by the width of its opening (shutter aperture), and how fast it closes again (shutter speed). Early cameras could not separate the three colors we perceive, reason why the first photographs were always in black and white. This was solved by the application of red, green and blue filters, which allowed to capture colors separately. This idea will helps us in the future when we try to visualize and interpret multi-spectral images. \n",
    "\n",
    "<img src=\"../images/color_photography.png\" alt=\"Color synthesis\" width=\"700\">\n",
    "\n",
    "_Three pictures taken by Mikhailovich Prokudin-Gorskii representing the red, green and blue color channels (left), and color composite generated from them (right). From [Wikipedia commons](https://commons.wikimedia.org/w/index.php?curid=464234)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Multi-spectral sensors\n",
    "\n",
    "Every band of a multi-spectral sensor measures the amount of energy received within a “bracket” or “band” of the electromagnetic spectrum. The number of these bands and how wide they are depends on the specific sensor. On the barest terms, the sensor for each band records the amount of energy received. Cameras have three bands (blue, green and red), commonly used multi-spectral sensors usually have 3-10 bands. The sensors we will be using for these notebooks are Landsat 8 and Sentinel-2, widely used in the field of multi-spectral remote sensing ([HLS product spectral bands](https://www.earthdata.nasa.gov/data/projects/hls/spectral-bands)).\n",
    "\n",
    "The energy recorded depends on the sensor aperture size, the integration time (think of shutter aperture and speed in a common camera), and its spectral sensitivity (it would look similar to the relative probability of absorption). Most of these parameters are known instrument characteristics, which are accounted for, allowing us to retrieve an estimate of the energy received: the spectral irradiance. [More details can be found here](https://www.cesbio.cnrs.fr/multitemp/radiometric-quantities-irradiance-radiance-reflectance/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Preparing the images for usage\n",
    "\n",
    "Going from satellite images as such to the images we analyze is a long process with many steps whose purpose are to ensure they are as comparable as possible. This section is not meant to go in full depth, only to briefly mention some concepts to make it easy to understand which products to use and what they account for. For starters, the images need to be geo-referenced, adding data indicating their position in the context of a coordinate reference system (CRS).\n",
    "\n",
    "The irradiance could be understood as something like the “Blue Marble” that astronauts saw from the orbit. However, for practical use the irradiance recorded by the sensor needs to be normalized to account for the sun’s emission across different wavelengths, converting the irradiance into reflectance: the fraction of the incoming radiation that is reflected (0-1 range). The result depicts what earth reflects for each wavelength, a quantity called top-of-atmosphere (TOA) reflectance, the standard used for Level-1 products.\n",
    "\n",
    "TOA reflectance would not suffice for our analyses, as the image would be affected by the earth, but also by the column of air that needs to be traversed twice to reach the sensor (sun-atmosphere-ground, ground-atmosphere-sensor). Atmospheric correction uses physical modeling to account for the impact of atmospheric gases (ozone, water vapor, etc.), going from top- to bottom-of atmosphere reflectance. Furthermore, multi-spectral sensors work on some of the wavelengths as our eye or close (ish-) to that region. We can see the clouds, and so can they!. For land applications clouds and their shadows need to be removed as they are extremely bright or dark compared with most of the land surfaces. Level-2 products have received both atmospheric corrections and are accompanied by cloud and shadow masks. \n",
    "\n",
    "Level-2 products are considered [analysis-ready](https://ceos.org/ard/files/PFS/SR/v5.0/CARD4L_Product_Family_Specification_Surface_Reflectance-v5.0.pdf). However, the product we use for our notebooks is a bit special, because it combines data from Landsat 8 and Sentinel-2 data so they can be used together as if the images came from a single sensor. The reason for this is that by using satellites from different programs it is possible to increase observation frequency, increasing our chances to “catch” cloudless observations. To make the observations more compatible the image grids are resampled to match, and reflectance is refined even further to suppress the impact of the combined effect of both illumination and the observation angles, which differ between Landsat and Sentinel-2. Finally, band-pass adjustment is applied to Sentinel-2 images so their reflectance matches the one of Landsat 8 even though the images were not acquired by the same sensor.\n",
    "\n",
    "<img src=\"../images/spectral_response.png\" alt=\"Spectral response\" width=\"700\">\n",
    "\n",
    "_Landsat 8/9 and Sentinel-2 A/B relative spectral responses (From [Lima et al, 2025](https://doi.org/10.1016/j.srs.2025.100225))_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Interpreting the Images\n",
    "Common digital images are displayed based on the three pixel values (ranged 0-255) contained in every pixel, which represent how much red, green and blue light should be emitted by the three channels of the display. Multi-spectral reflectance images (0-1 range) can be displayed in the same way, in what we call a true color composite. However, if we remain within the confines of the wavelengths perceived by our eye, we would not see what the other bands have to offer. What we can do is to place swap our usual red-green-blue with other bands, putting wavelengths invisible to our eye where it can perceive them. For example, in the following figure we can see two photos of the same area. The first is in natural color, and the second places the near-infrared on the red channel, whereas the red is displaced to the green channel, and green to the blue channel. This is what is called a [false or synthetic color composite](https://earthobservatory.nasa.gov/features/FalseColor). Now, the question is: which information we can get from every band?\n",
    "\n",
    "True color composite from August 2005.<br><br>Red, Green and blue are assigned to their homonyms channels.<br><br>Credit: [Mike Murphy / Wikipedia commons](https://commons.wikimedia.org/wiki/File:Half_dome_yosemite_national_park.jpg).|MSS False color composite from May 1972.<br><br>NIR on the red channel, red on the green and, and green on the blue.<br><br>Credit: [Hughes Aircraft Company / NASA](https://landsat.gsfc.nasa.gov/article/1972-mss-image-of-half-dome/)\n",
    ":-------------------------:|:-------------------------:\n",
    "![](../images/halfDome_natural.png)  |  ![](../images/halfDome_false.png)\n",
    "\n",
    "_Two photographs of the Half dome of Yosemite National Park. Both were taken from Glacier Point._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## What is shown by every band: the spectral signature\n",
    "\n",
    "The interaction of light with any kind of surface depends on the wavelength of the light, and the physical and chemical properties of the surface. Dry soils tend to be very bright, darkening with an increasing water content. Water reflects very little on the visible spectrum (red, green, blue), with clearer waters having lower reflectances. Towards the near and shortwave infrared the reflectance is nearly 0. The case of vegetation is very interesting the chlorophyll in the plants absorbs blue and red, whereas it reflects the green, the color we perceive. Over the near infrared (NIR) plants are very reflective/bright due leaf structure, whereas on the shortwave infrared (SWIR1) the leaves are significantly less reflective due the water absorption: the moister the leaves are, the darker they get.\n",
    "\n",
    "<img src=\"../images/spectral_signature.png\" alt=\"Spectral signatures\" width=\"700\">\n",
    "\n",
    "_Reflectance of some targets across different wavelengths. Modified from “Introduction to hyperspectral imaging” ([Smith , 2012, ©MicroImages, Inc.](https://www.microimages.com/documentation/Tutorials/hyprspec.pdf))_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray  # noqa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from envrs.download_path import make_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the cube\n",
    "cube_uri = make_url(\"HLS_clip4plots_both_b30_v20.zarr.zip\", is_zip=True)\n",
    "\n",
    "full_cube = xr.open_dataset(\n",
    "    cube_uri,\n",
    "    engine=\"zarr\",\n",
    "    consolidated=False,\n",
    ").compute()\n",
    "\n",
    "# Read the points, reproject tro match the datacube, set the index\n",
    "points_uri = make_url(\"timeline_points.geojson\")\n",
    "points = (\n",
    "    gpd.read_file(points_uri)\n",
    "    .to_crs(full_cube[\"spatial_ref\"].attrs[\"crs_wkt\"])\n",
    "    .set_index(\"intact\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Specify the variables indicating cloud/shadow presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "cirrus_col = \"cirrus cloud\"\n",
    "cloud_col = \"cloud\"\n",
    "adjacent_col = \"adjacent to cloud\"\n",
    "shadow_col = \"cloud shadow\"\n",
    "tainted_cols = [adjacent_col, cloud_col, cirrus_col, shadow_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Select the clearest observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tainted_frame = (\n",
    "    full_cube[tainted_cols]\n",
    "    .to_dataarray(dim=\"mask\")\n",
    "    .any(dim=\"mask\")\n",
    "    .sum(dim=[\"x\", \"y\"])\n",
    "    .to_dataframe(name=\"count_tainted\")\n",
    "    .sort_values(by=\"count_tainted\")\n",
    ")\n",
    "\n",
    "monthly_clearest = tainted_frame.groupby(pd.Grouper(freq=\"ME\")).head(1)\n",
    "monthly_cube = full_cube.sel(time=monthly_clearest.index.values)\n",
    "\n",
    "fully_clear = tainted_frame[tainted_frame[\"count_tainted\"] == 0]\n",
    "clear_cube = full_cube.sel(time=fully_clear.index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## How an image time series looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Images need to be converted to 8 bits to set the color stretch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_rgb8(cube, r, g, b, vmax, vmin=0):\n",
    "    selected = cube[[r, g, b]].to_dataarray(\"band\")\n",
    "    stretched = (selected - vmin) / (vmax - vmin)\n",
    "\n",
    "    is_positive = (stretched >= 0) & np.isfinite(stretched)\n",
    "    positive = stretched.where(is_positive.all(dim=\"band\"), 0)\n",
    "    clipped = (\n",
    "        np.clip(255 * positive, 0, 255)\n",
    "        .astype(np.uint8)\n",
    "        .expand_dims({\"composite\": [f\"{r=}, {g=}, {b=}\"]})\n",
    "    )\n",
    "    clipped[\"band\"] = np.array([\"r\", \"g\", \"b\"], dtype=\"unicode\")\n",
    "\n",
    "    return clipped\n",
    "\n",
    "\n",
    "def plot_rgb(*args, dimname=\"composite\"):\n",
    "    return xr.concat(args, dim=dimname).hvplot.rgb(\n",
    "        x=\"x\",\n",
    "        y=\"y\",\n",
    "        bands=\"band\",\n",
    "        by=dimname,\n",
    "        groupby=\"time\",\n",
    "        subplots=True,\n",
    "        rasterize=True,\n",
    "        data_aspect=1,\n",
    "        xaxis=False,\n",
    "        yaxis=None,\n",
    "        widget_location=\"bottom\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Images over time\n",
    "\n",
    "The widgets on the following two code cells show some images over the same area. The first cell includes images that can go from fully clear to fully covered by clouds, just to show how much of a problem clouds can be, whereas the second shows images clouds have not been detected. Both cells depict the same forest area in both true color (red, green, blue; on the right) and false color (left, NIR on the red channel, SWIR1 on the green and red on the blue channel). At the start of the year the false color composite looks orange/red, whereas the true color composite looks green. The former is caused by the intense reflection caused by leaf structure, whereas the latter is driven by leaf pigmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Cloudy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with clouds\n",
    "plot_rgb(\n",
    "    to_rgb8(monthly_cube, r=\"Red\", g=\"Green\", b=\"Blue\", vmax=0.15),\n",
    "    to_rgb8(monthly_cube, r=\"NIRnarrow\", g=\"SWIR1\", b=\"Red\", vmax=0.40),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Clear\n",
    "\n",
    "As the year advances the false color composite loses its reddish coloration and turns greener and greener, indicating leaf structure is not so reflective anymore, and a decrease of leaf water content. The true color composite shows a similar pattern, with most areas going from green to brown. Only the valleys retain the same color, possibly because these areas tend to gather water from the surrounding slopes, which would allow vegetation to better withstand a dry season. Around September/October the redness of the false color composite and the greenness of the true color composite start to increase again. This cycle could be explained by the stress induced by a dry season. Another interesting pattern is how the shading created by the terrain varies following the sun declination, an effect that may need to be corrected for some applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rgb(\n",
    "    to_rgb8(clear_cube, r=\"Red\", g=\"Green\", b=\"Blue\", vmax=0.15),\n",
    "    to_rgb8(clear_cube, r=\"NIRnarrow\", g=\"SWIR1\", b=\"Red\", vmax=0.40),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "The animation not only shows the natural patterns, but also tree cover losses. At the end of November 2019 we see the appearance of a patch with an unusual color for this time of the year. Even though by March/April of 2020 it has regained its red/green color, it remains paler, which may indicate shorter vegetation has substituted the trees. This process reappears in May 2021, where we see how the deforested area suffers a large increase. At first these areas are dissimilar to the surrounding vegetation, and some regain a reddish/orange (false color) or pale green (true color) tone on the composites. Combined with the spatial pattern of the patches we could interpret that these areas are being cultivated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Satellite pixel time series as tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Pick two points one that has been deforested, and one that is intact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://tutorial.xarray.dev/intermediate/indexing/advanced-indexing.html\n",
    "# #orthogonal-indexing-in-xarray\n",
    "sel_cube = full_cube.sel(\n",
    "    x=xr.DataArray(points.geometry.x, dims=\"intact\"),\n",
    "    y=xr.DataArray(points.geometry.y, dims=\"intact\"),\n",
    "    method=\"nearest\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Flatten (xarray dataset/\"cube\" to pandas dataframe/table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_frame = (\n",
    "    sel_cube.drop_vars(\"spatial_ref\")\n",
    "    .to_dataframe()\n",
    "    .drop(columns=[\"x\", \"y\"])\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Prepare auxiliary variables for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_difference(frame, positive, negative):\n",
    "    numerator = frame[positive] - frame[negative]\n",
    "    denominator = frame[positive] + frame[negative]\n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "sel_frame[\"NDVI\"] = normalized_difference(sel_frame, \"NIRnarrow\", \"Red\")\n",
    "sel_frame[\"NDMI\"] = normalized_difference(sel_frame, \"NIRnarrow\", \"SWIR1\")\n",
    "sel_frame[\"DOY\"] = sel_frame[\"time\"].dt.dayofyear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### Split the table into intact and deforested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "deforested_frame = sel_frame[~sel_frame[\"intact\"]]\n",
    "intact_frame = sel_frame[sel_frame[\"intact\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## A single pixel pixel time\n",
    "\n",
    "### A cloudy pixel over time\n",
    "\n",
    "The widget under this lines shows the temporal behavior of the green band reflectance over a single forested pixel. In this specific case, cloudy observations (black dots) have not been masked. As you can see, clouds are extremely reflective, to a point they dwarf everything else. If we take a close look on the clear observations (green), we can see that the cycle we mentioned on the previous section is still there, but is dwarfed by the impact of clouds and shadows. This plot is also meant to underline why most of the time it may be reasonable to mask pixels within certain distance to a cloud. These could be fine (they were not detected as cloud after all), but it also can happen that even though that the value is not extreme, it may be abnormally bright/dark, reason why these pixels may need to be masked as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "intact_frame.loc[:, \"flag\"] = \"clear\"\n",
    "intact_frame.loc[intact_frame[tainted_cols[1:]].any(axis=1), \"flag\"] = \"cloud/shadow\"\n",
    "intact_frame.loc[intact_frame[tainted_cols[0]], \"flag\"] = \"adjacent\"\n",
    "\n",
    "\n",
    "intact_frame.hvplot.scatter(\n",
    "    x=\"time\", y=\"Green\", by=\"flag\", color=[\"green\", \"black\", \"orange\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### Filter out flagged observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "intact_masked = intact_frame[~intact_frame[tainted_cols].any(axis=1)].copy()\n",
    "deforested_masked = deforested_frame[~deforested_frame[tainted_cols].any(axis=1)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Missed clouds as spikes\n",
    "\n",
    "If we plot the band values for the “clear” observations over our forest pixel now we have a much clearer picture of the temporal behavior. However, that does not necessarily mean that all cloudy or shaded observations have been removed. For example, on the 13th of July 2018 we can see that there was a massive downwards spike that is very likely to be a shaded observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "tetracolor_kwargs = {\n",
    "    \"x\": \"time\",\n",
    "    \"y\": [\"Blue\", \"Green\", \"Red\", \"NIRnarrow\"],\n",
    "    \"color\": [\"blue\", \"green\", \"red\", \"darkgray\"],\n",
    "}\n",
    "\n",
    "intact_masked.hvplot(**tetracolor_kwargs)  # .legend(loc=\"upper left\", ncols=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "A possible solution ([Timesat manual, pages 14-15](https://web.nateko.lu.se/timesat/docs/TIMESAT33_SoftwareManual.pdf)) to de-spike our time series is to calculate the absolute difference between every observations (at time t), and the mean of its immediate neighborhood (previous, t-1 and next t+1). In our case our function sums up all the reflectances because this workflow is based on a single value, and then discards the observations with the largest vertical distance respective to their temporal neighbors (5% lowest and the 5% highest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def despike(frame, columns, min_spike, max_spike):\n",
    "    summed = frame[columns].sum(axis=1)\n",
    "\n",
    "    # Perform the selections\n",
    "    central = summed.iloc[1:-1]\n",
    "    prior = summed.shift(-1).iloc[1:-1]\n",
    "    posterior = summed.shift(1).iloc[1:-1]\n",
    "\n",
    "    # remove observations based on their saliency respective to their neighbors\n",
    "    spikyness = central - (prior + posterior) / 2\n",
    "    floor, ceiling = spikyness.quantile((min_spike, max_spike))\n",
    "    selected = central[spikyness.between(floor, ceiling)]\n",
    "\n",
    "    return frame.loc[selected.index]\n",
    "\n",
    "\n",
    "cutoff = 0.05\n",
    "band_names = [\"Blue\", \"Green\", \"Red\", \"NIRnarrow\", \"SWIR1\", \"SWIR2\"]\n",
    "\n",
    "intact_despiked = despike(intact_masked, band_names, cutoff, 1 - cutoff)\n",
    "deforested_despiked = despike(deforested_masked, band_names, cutoff, 1 - cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    intact_masked.hvplot(x=\"time\", y=\"NIRnarrow\", color=\"k\")\n",
    "    * intact_despiked.hvplot(x=\"time\", y=\"NIRnarrow\", color=\"darkgray\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### Another view of the temporal behavior\n",
    "\n",
    "So far we have been plotting using the time as x, and the reflectances as y. However, another possibility is to assign x and y to specific bands and use the point color to represent the position within the yearly cycle (day of the year, 1-365). On the plot under these lines we can see that at the start of the year the NIR value is high (leaf structure reflectance), and red is low (chlorophyll absorption). Over the year the NIR decreases and red increases, which could indicate that leaves have been shed if the trees are deciduous (loss of leaf structure and chlorophyll absorption). Past day 250 NIR increase and red decreases, a trend that continues until looping back to the initial position. This plot also is interesting to show that outliers (crossed out observations) may be very far away from the rest of the distribution, but some of them just seemed abnormal based on their specific neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_frame = intact_masked.iloc[1:-1].drop(index=intact_despiked.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOY = day of the year\n",
    "(\n",
    "    intact_masked.hvplot.scatter(x=\"Red\", y=\"NIRnarrow\", c=\"DOY\", colormap=\"twilight\")\n",
    "    * spike_frame.hvplot.scatter(x=\"Red\", y=\"NIRnarrow\", marker=\"x\", s=90, color=\"red\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "## How deforestation may look like\n",
    "\n",
    "### On the bands \n",
    "\n",
    "Once we have double-checked clouds have been removed, we can compare the time series of a plot where trees have not been disturbed, and another where the trees have been removed. In the case of “intact” (difficult to know if that’s actually the case) we see this nice periodic cycle that we have been describing so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "intact_despiked.hvplot(**tetracolor_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "In the deforested plot we see a similar pattern until April 2022, where the behavior starts to change. The NIR reflectance decreases sooner than expected and keeps dropping until reaching the minimum of the time series, possibly due the removal of the tree canopies (mostly the leaves they hold). Past the minimum, blue, green and red seem to oscillate around higher values, which would be explained by a higher exposure of the soil. Past the downward spike the NIR reflectance seems to have a new cycle with two peaks per year instead of a single one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "deforested_despiked.hvplot(**tetracolor_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "### On the indices\n",
    "\n",
    "Several spectral bands can be combined to highlight the presence of specific targets behaviors.  There is a [massive variety of spectral indices](https://doi.org/10.1038/s41597-023-02096-0) specialized to highlight a wide array of phenomena: [water presence, bare soil, burned area, mineral identification, etc.](https://appliedsciences.nasa.gov/sites/default/files/2025-02/Spectral_Indices_QGIS.pdf). The normalized difference vegetation index is one of the earlier and most famous examples. It uses the reflectances of the NIR and red to describe plant health.\n",
    "\n",
    "$$\n",
    "NDVI = \\frac{NIR - red}{NIR + red}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_frame = (\n",
    "    pd.concat({\"deforested\": deforested_despiked, \"intact\": intact_despiked}, axis=0)\n",
    "    .reset_index(names=[\"history\", \"index\"])\n",
    "    .drop(columns=\"index\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "In the context of our deforestation example we can see the drop in vegetation health, and, even if it raises again, it remains at lower values than in the case of the intact plot. The shape of the cycle also is different with two sharp spikes instead of the plateau of intact forest, which could be indicative of crop presence with two yearly cropping cycles.\n",
    "\n",
    "One important takeaway from these plot is that deforestation may alter the pixel spectro-temporal signature, but that does not necessarily mean that plant health or phenology will not show up again. And when it shows up again is important to understand that that does not mean that forest has recovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_frame.hvplot.line(\n",
    "    x=\"time\", y=\"NDVI\", by=\"history\", color=[\"black\", \"limegreen\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environmental-remote-sensing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
