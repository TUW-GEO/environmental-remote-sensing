{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Deforestation detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8391a8",
   "metadata": {},
   "source": [
    "# An excursion into Multispectral Remote Sensing Basics\n",
    "## The electromagnetic spectrum\n",
    "What we perceive as light is just a small portion of the full electromagnetic spectrum. It is organized by wavelength (λ), which control the interaction with matter. Different sensors use different parts of the spectrum for specific applications: microwave remote sensing uses wavelengths on a centimeter scale to analyze soil water content, and thermal remote sensing estimates land surface temperature with wavelengths on the micrometer scale (10-6 m) to estimate. Multi-spectral sensors register the energy in frequency bands whose wavelengths are described on a micrometer or a nanometer scale (10-9 m).\n",
    "The source of the energy received by the sensor depends on the wavelength it records. Microwave sensors can generate their own illumination (active; scatterometers, synthetic aperture radar), or register the energy emitted by the earth (passive; e.g., radiometers). Thermal and multi-spectral sensors are passive, with the former registering the amount of energy emitted by the earth, and the latter, which registers how much of the sun’s energy is reflected by the Earth.\n",
    "\n",
    "The most used regions on multi-spectral remote sensing are the visible region (0.4 – 0.7 μm) is the part of the spectrum our eyes can perceive. As wavelength increases we veer into the infrared, region, which usually is separated into near-infrared (NIR) and middle- or shortwave infrared (SWIR). Over the next section we will see what is the usefulness of these bands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3854d1",
   "metadata": {},
   "source": [
    "## An intuitive view of multi-spectral remote sensing\n",
    "### The human eye as a sensor\n",
    "\n",
    "The human eye contains two types of light-sensitive the cells: rods and cones. The former are concerned with lightness and motion, whereas the latter have tree types: red-, green- and blue- receptive cones. Each type of cell is sensitive to a specific portion of the spectrum, with a peak where sensitivity is at it highest, and areas where the cell is not sensitive at all. Having these three types of cells allows us to perceive red, green, blue and their mixtures. Other animals can see fewer (e.g., the dogs, 2) or more colors (e.g., birds such as the European starling, 4).\n",
    "\n",
    "<!-- NOTE: to the developer... stock photos have been added, real ones need to be added later -->\n",
    "![Image](https://picsum.photos/500/300)\n",
    "\n",
    "On the image you can see the rod sensitivities of the of human and an European Starling. Color sensitivity as a concept extends to the next closest example: the cameras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0d2df9",
   "metadata": {},
   "source": [
    "## Commom Cameras\n",
    "Cameras use a lens to focus illumination reflected by the objects onto a sensor (digital) or film (analog). In both cases, a shutter controls how much energy enters by the width of its opening (shutter aperture), and how fast it closes again (shutter speed). Early cameras could not separate the three colors we perceive, reason why the first photographs were always in black and white. This was solved by the application of red, green and blue filters, which allowed to capture colors separately. This idea will helps us in the future when we try to visualize and interpret multi-spectral images. \n",
    "\n",
    "![Image](https://picsum.photos/510/310)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb0cbf3",
   "metadata": {},
   "source": [
    "## Multi-spectral sensors\n",
    "\n",
    "Every band of a multi-spectral sensor measures the amount of energy received within a “bracket” or “band” of the electromagnetic spectrum. The number of these bands and how wide they are depends on the specific sensor. On the barest terms, the sensor for each band records the amount of energy received. Cameras have three bands (blue, green and red), commonly used multi-spectral sensors usually have 3-10 bands. The sensors we will be using for these notebooks are Landsat 8 and Sentinel-2, widely used in the field of multi-spectral remote sensing (see table).\n",
    "\n",
    "The energy recorded depends on the sensor aperture size, the integration time (think of shutter aperture and speed in a common camera), and its spectral sensitivity (it would look similar to the relative probability of absorption). Most of these parameters are known instrument characteristics, which are accounted for, allowing us to retrieve an estimate of the energy received: the spectral radiance. [Source]\n",
    "\n",
    "[source]: https://www.cesbio.cnrs.fr/multitemp/radiometric-quantities-irradiance-radiance-reflectance/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60370105",
   "metadata": {},
   "source": [
    "## Preparing the images for usage\n",
    "Going from satellite images as such to the images we analyze is a long process with many steps whose purpose are to ensure they are as comparable as possible. This section is not meant to go in full depth, only to briefly mention some concepts to make it easy to understand which products to use and what they account for. For starters, the images need to be geo-referenced, adding data indicating their position in the context of a coordinate reference system (CRS).\n",
    "The irradiance could be understood as something like the “Blue Marble” that astronauts saw from the orbit. However, for practical use the irradiance recorded by the sensor needs to be normalized to account for the sun’s emission across different wavelengths, converting the irradiance into reflectance: the fraction of the incoming radiation that is reflected (0-1 range). The result depicts what earth reflects for each wavelength, a quantity called top-of-atmosphere (TOA) reflectance, the standard used for Level-1 products.\n",
    "TOA reflectance would not suffice for our analyses, as the image would be affected by the earth, but also by the column of air that needs to be traversed twice to reach the sensor (sun-atmosphere-ground, ground-atmosphere-sensor). Atmospheric correction uses physical modeling to account for the impact of atmospheric gases (ozone, water vapor, etc.), going from top- to bottom-of atmosphere reflectance. Furthermore, multi-spectral sensors work on some of the wavelengths as our eye or close (ish-) to that region. We can see the clouds, and so can they!. For land applications clouds and their shadows need to be removed as they are extremely bright or dark compared with most of the land surfaces. Level-2 products have received both atmospheric corrections and are accompanied by cloud and shadow masks. \n",
    "\n",
    "[source](https://ceos.org/ard/files/PFS/SR/v5.0/CARD4L_Product_Family_Specification_Surface_Reflectance-v5.0.pdf)\n",
    "\n",
    "Level-2 products are considered analysis-ready. However, the product we use for our notebooks is a bit special, because it combines data from Landsat 8 and Sentinel-2 data so they can be used together as if the images came from a single sensor. The reason for this is that by using satellites from different programs it is possible to increase observation frequency, increasing our chances to “catch” cloudless observations. To make the observations more compatible the image grids are resampled to match, and reflectance is refined even further to suppress the impact of the combined effect of both illumination and the observation angles, which differ between Landsat and Sentinel-2. Finally, band-pass adjustment is applied to Sentinel-2 images so their reflectance matches the one of Landsat 8 even though the images were not acquired by the same sensor.\n",
    "\n",
    "\n",
    "![Image](https://picsum.photos/800/250)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ace76c1",
   "metadata": {},
   "source": [
    "## Mosaics\n",
    "Note that even though the processing algorithms for optical data have been designed with great care, the images still can contain imperfections, such as clouds or shadows that have not been masked. And even if they are masked, we could be left with images that have massive gaps! A common solution to solve both the gaps and imperfections is the creation of multi-temporal mosaics. This term covers various techniques designed to “patch” areas covered by clouds/shadows whilst trying to leave out the artifacts. On simple terms, is using several observation so the “holes” in one image are filled by another. A common technique to create annual mosaics is to calculate of annual quantiles. This specific statistic is chosen because it is just concerned about the order, ensuring artifacts are left in a very low (e.g., 0 or 0.05, shadows) or a very high quantile (e.g., 0.95, 1.0, clouds). However, statistics like the mean could be tainted by outliers as they are affected by all the observations. Mosaics are Level-3 products, periodic (statistical) summaries.\n",
    "\n",
    "[source](https://www.earthdata.nasa.gov/learn/earth-observation-data-basics/data-processing-levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc36e66a",
   "metadata": {},
   "source": [
    "## Interpreting the Images\n",
    "Common digital images are displayed based on the three pixel values (ranged 0-255) contained in every pixel, which represent how much red, green and blue light should be emitted by the three channels of the display. Multi-spectral reflectance images (0-1 range) can be displayed in the same way, in what we call a true color composite. However, if we remain within the confines of the wavelengths perceived by our eye, we would not see what the other bands have to offer. What we can do is to place swap our usual red-green-blue with other bands, putting wavelengths invisible to our eye where it can perceive them. For example, in the following figure we can see two photos of the same area. The first is in natural color, and the second places the near-infrared on the red channel, whereas the red is displaced to the green channel, and green to the blue channel. This is what is called a false or synthetic color composite. Now, the question is: which information we can get from every band?\n",
    "\n",
    "![image](https://picsum.photos/800/400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3c60c9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14d2c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ee0b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray  # noqa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from envrs.download_path import make_url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the cube\n",
    "cube_uri = make_url(\"HLS_clip4plots_both_b30_v20.zarr.zip\", is_zip=True)\n",
    "\n",
    "full_cube = xr.open_dataset(\n",
    "    cube_uri,\n",
    "    engine=\"zarr\",\n",
    "    consolidated=False,\n",
    ").compute()\n",
    "\n",
    "# Read the points, reproject tro match the datacube, set the index\n",
    "points_uri = make_url(\"timeline_points.geojson\")\n",
    "points = (\n",
    "    gpd.read_file(points_uri)\n",
    "    .to_crs(full_cube[\"spatial_ref\"].attrs[\"crs_wkt\"])\n",
    "    .set_index(\"intact\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Specify the variables indicating cloud/shadow presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cirrus_col = \"cirrus cloud\"\n",
    "cloud_col = \"cloud\"\n",
    "adjacent_col = \"adjacent to cloud\"\n",
    "shadow_col = \"cloud shadow\"\n",
    "tainted_cols = [adjacent_col, cloud_col, cirrus_col, shadow_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Select the clearest observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tainted_frame = (\n",
    "    full_cube[tainted_cols]\n",
    "    .to_dataarray(dim=\"mask\")\n",
    "    .any(dim=\"mask\")\n",
    "    .sum(dim=[\"x\", \"y\"])\n",
    "    .to_dataframe(name=\"count_tainted\")\n",
    "    .sort_values(by=\"count_tainted\")\n",
    ")\n",
    "\n",
    "monthly_clearest = tainted_frame.groupby(pd.Grouper(freq=\"ME\")).head(1)\n",
    "monthly_cube = full_cube.sel(time=monthly_clearest.index.values)\n",
    "\n",
    "fully_clear = tainted_frame[tainted_frame[\"count_tainted\"] == 0]\n",
    "clear_cube = full_cube.sel(time=fully_clear.index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## How an image time series looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Images need to be converted to 8 bits to set the color stretch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_rgb8(cube, r, g, b, vmax, vmin=0):\n",
    "    selected = cube[[r, g, b]].to_dataarray(\"band\")\n",
    "    stretched = (selected - vmin) / (vmax - vmin)\n",
    "\n",
    "    is_positive = (stretched >= 0) & np.isfinite(stretched)\n",
    "    positive = stretched.where(is_positive.all(dim=\"band\"), 0)\n",
    "    clipped = (\n",
    "        np.clip(255 * positive, 0, 255)\n",
    "        .astype(np.uint8)\n",
    "        .expand_dims({\"composite\": [f\"{r=}, {g=}, {b=}\"]})\n",
    "    )\n",
    "    clipped[\"band\"] = np.array([\"r\", \"g\", \"b\"], dtype=\"unicode\")\n",
    "\n",
    "    return clipped\n",
    "\n",
    "\n",
    "def plot_rgb(*args, dimname=\"composite\"):\n",
    "    return xr.concat(args, dim=dimname).hvplot.rgb(\n",
    "        x=\"x\",\n",
    "        y=\"y\",\n",
    "        bands=\"band\",\n",
    "        by=dimname,\n",
    "        groupby=\"time\",\n",
    "        subplots=True,\n",
    "        rasterize=True,\n",
    "        data_aspect=1,\n",
    "        xaxis=False,\n",
    "        yaxis=None,\n",
    "        widget_location=\"bottom\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Plot a time series of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with clouds\n",
    "plot_rgb(\n",
    "    to_rgb8(monthly_cube, r=\"Red\", g=\"Green\", b=\"Blue\", vmax=0.15),\n",
    "    to_rgb8(monthly_cube, r=\"NIRnarrow\", g=\"SWIR1\", b=\"Red\", vmax=0.40),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Plot just the \"fully\" clear images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rgb(\n",
    "    to_rgb8(clear_cube, r=\"Red\", g=\"Green\", b=\"Blue\", vmax=0.15),\n",
    "    to_rgb8(clear_cube, r=\"NIRnarrow\", g=\"SWIR1\", b=\"Red\", vmax=0.40),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Satellite pixel time series as tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Pick two points one that has been deforested, and one that is intact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://tutorial.xarray.dev/intermediate/indexing/advanced-indexing.html\n",
    "# #orthogonal-indexing-in-xarray\n",
    "sel_cube = full_cube.sel(\n",
    "    x=xr.DataArray(points.geometry.x, dims=\"intact\"),\n",
    "    y=xr.DataArray(points.geometry.y, dims=\"intact\"),\n",
    "    method=\"nearest\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Flatten (xarray dataset/\"cube\" to pandas dataframe/table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_frame = (\n",
    "    sel_cube.drop_vars(\"spatial_ref\")\n",
    "    .to_dataframe()\n",
    "    .drop(columns=[\"x\", \"y\"])\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Prepare auxiliary variables for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_difference(frame, positive, negative):\n",
    "    numerator = frame[positive] - frame[negative]\n",
    "    denominator = frame[positive] + frame[negative]\n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "sel_frame[\"NDVI\"] = normalized_difference(sel_frame, \"NIRnarrow\", \"Red\")\n",
    "sel_frame[\"NDMI\"] = normalized_difference(sel_frame, \"NIRnarrow\", \"SWIR1\")\n",
    "sel_frame[\"DOY\"] = sel_frame[\"time\"].dt.dayofyear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Split the table into intact and deforested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "deforested_frame = sel_frame[~sel_frame[\"intact\"]]\n",
    "intact_frame = sel_frame[sel_frame[\"intact\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Clouds and shadows taint our time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "intact_frame.loc[:, \"flag\"] = \"clear\"\n",
    "intact_frame.loc[intact_frame[tainted_cols[1:]].any(axis=1), \"flag\"] = \"cloud/shadow\"\n",
    "intact_frame.loc[intact_frame[tainted_cols[0]], \"flag\"] = \"adjacent\"\n",
    "\n",
    "\n",
    "intact_frame.hvplot.scatter(\n",
    "    x=\"time\", y=\"Green\", by=\"flag\", color=[\"green\", \"black\", \"orange\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Filter out flagged observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "intact_masked = intact_frame[~intact_frame[tainted_cols].any(axis=1)].copy()\n",
    "deforested_masked = deforested_frame[~deforested_frame[tainted_cols].any(axis=1)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Removed the cloud, the timeline becomes clearer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tetracolor_kwargs = {\n",
    "    \"x\": \"time\",\n",
    "    \"y\": [\"Blue\", \"Green\", \"Red\", \"NIRnarrow\"],\n",
    "    \"color\": [\"blue\", \"green\", \"red\", \"darkgray\"],\n",
    "}\n",
    "\n",
    "intact_masked.hvplot(**tetracolor_kwargs)  # .legend(loc=\"upper left\", ncols=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Spikes could be unmasked clouds/shadows\n",
    "\n",
    "A massive value difference respective to its neighbors indicates the presence of possible outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def despike(frame, columns, min_spike, max_spike):\n",
    "    summed = frame[columns].sum(axis=1)\n",
    "\n",
    "    # Perform the selections\n",
    "    central = summed.iloc[1:-1]\n",
    "    prior = summed.shift(-1).iloc[1:-1]\n",
    "    posterior = summed.shift(1).iloc[1:-1]\n",
    "\n",
    "    # remove observations based on their saliency respective to their neighbors\n",
    "    spikyness = central - (prior + posterior) / 2\n",
    "    floor, ceiling = spikyness.quantile((min_spike, max_spike))\n",
    "    selected = central[spikyness.between(floor, ceiling)]\n",
    "\n",
    "    return frame.loc[selected.index]\n",
    "\n",
    "\n",
    "cutoff = 0.05\n",
    "band_names = [\"Blue\", \"Green\", \"Red\", \"NIRnarrow\", \"SWIR1\", \"SWIR2\"]\n",
    "\n",
    "intact_despiked = despike(intact_masked, band_names, cutoff, 1 - cutoff)\n",
    "deforested_despiked = despike(deforested_masked, band_names, cutoff, 1 - cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    intact_masked.hvplot(x=\"time\", y=\"NIRnarrow\", color=\"k\")\n",
    "    * intact_despiked.hvplot(x=\"time\", y=\"NIRnarrow\", color=\"darkgray\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## The spikes as anomalies\n",
    "\n",
    "These spikes are anolalies on the context where they appear, but they may or\n",
    "may not be global outliers when compared with the full population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_frame = intact_masked.iloc[1:-1].drop(index=intact_despiked.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOY = day of the year\n",
    "(\n",
    "    intact_masked.hvplot.scatter(x=\"Red\", y=\"NIRnarrow\", c=\"DOY\", colormap=\"twilight\")\n",
    "    * spike_frame.hvplot.scatter(x=\"Red\", y=\"NIRnarrow\", marker=\"x\", s=90, color=\"red\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "The isolated points far above and below the general population would be\n",
    "global outliers, whereas the rest were outliers on their specific context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "## The signature of deforestation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "intact_despiked.hvplot(**tetracolor_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "deforested_despiked.hvplot(**tetracolor_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## With Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_frame = (\n",
    "    pd.concat({\"deforested\": deforested_despiked, \"intact\": intact_despiked}, axis=0)\n",
    "    .reset_index(names=[\"history\", \"index\"])\n",
    "    .drop(columns=\"index\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_frame.hvplot.line(\n",
    "    x=\"time\", y=\"NDVI\", by=\"history\", color=[\"black\", \"limegreen\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
