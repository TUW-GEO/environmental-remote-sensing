{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Validation of Remotely Sensed Droughts\n",
    "**Comparing Spatial Trends**\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook we continue with drought indicators, as introduced in notebook1. The h SAF ASCAT drought indicator is based on soil moisture (temporal) anomalies, which require a long-term mean for a robust measure. The anomalies are than calculated as standardized deviations from the long-term mean.\n",
    "\n",
    "\n",
    "AS explained in the first notebook, ASCAT SSM comes in units degree of saturation. Although spatial patterns,... , relative changes as used for drought anomaly detection\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import datashader as ds\n",
    "import holoviews as hv\n",
    "import hvplot.pandas  # noqa\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Standardized Precipitation-Evapotranspiration Index\n",
    "\n",
    "Historically, many studies on drought monitoring have used hydro-meteorological indicators such as the Standardized Precipitation Index (SPI) and the more comprehensive Standardized Precipitation-Evapotranspiration Index (SPEI). However, satellite-based soil moisture estimates and linked drought anomaly indices might be more suitable for agricultural drought monitoring as they focus more directly on plant water requirements.\n",
    "\n",
    "Despite this, SPEI indirectly accounts for soil conditions by combining both precipitation and potential evapotranspiration (PET), which measures atmospheric demand for water. Both SPEI and standardized soil moisture drought anomalies, such as Z scores, allow for comparisons across different regions and time scales.\n",
    "\n",
    "To summarize:\n",
    "\n",
    "- SPEI is a hydro-meteorological indicator for drought monitoring and thus indirectly measures soil moisture.\n",
    "- Z scores are based on standardized anomalies derived from spaceborne (H SAF ASCAT) microwave-retrieved soil moisture data for the upper ~5 cm.\n",
    "\n",
    "The purpose of this notebook is to compare both drought indicators, particularly focusing on the spatial extent of drought-affected areas. We will relate these indicators to recorded drought events in Mozambique between 2007 and 2021 ([The International Disaster Database](https://www.emdat.be/)).\n",
    "\n",
    "| **Year/Period**       | **Affected Provinces**                                     | **Estimated Affected People**              |\n",
    "|-----------------------|-------------------------------------------------------------|---------------------------------------------|\n",
    "| **2008 (Early)**      | Maputo, Gaza, Inhambane, Manica, Sofala, Tete               | ~1.2 million         |\n",
    "| **2010**              | Maputo, Gaza, Inhambane                                     | ~460,000             |\n",
    "| **2015–2016 (El Niño)**| Maputo, Gaza, Inhambane, Sofala, Tete                      | ~2.3 million         |\n",
    "| **2021**              | Cabo Delgado, Tete (south), Manica (north)                  | ~1.56 million        |\n",
    "\n",
    "\n",
    "We first load the SPEI data in the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./src/download_path.py\n",
    "\n",
    "url = make_url(\"spei-6_25_monthly.csv\")  # noqa\n",
    "df1 = pd.read_csv(\n",
    "    url,\n",
    "    index_col=[\"time\", \"location_id\"],\n",
    "    parse_dates=[\"time\"],\n",
    ")\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Now let's also load the H SAF ASCAT surface soil moisture data which we used in notebook 1, but we will filter the data to have a comparable time range (up to the end of 2021) as we have for the SPEI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = make_url(\"ascat-6_25_ssm_monthly.csv\")  # noqa\n",
    "df2 = pd.read_csv(\n",
    "    url,\n",
    "    index_col=[\"time\", \"location_id\"],\n",
    "    parse_dates=[\"time\"],\n",
    ")[[\"zscore\"]]\n",
    "df2 = df2[df2.index <= df1.index.max()]\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Merging Drought Indicator DATA\n",
    "\n",
    "Now we will have to combine both datasets. Fortunately, both datasets are already projected on the same grid, so this task does not involve any reprojection and resampling of the data. We can combine the data with a SQL-style left join. A left join in pandas merges two DataFrames while keeping all rows from the left DataFrame and adding matching rows from the right DataFrame. If there's no match, it fills with NaN (see the figure for a schematic overview).\n",
    "\n",
    "![Source: Ppandas User Guide](https://pandas.pydata.org/docs/user_guide)](https://pandas.pydata.org/docs/_images/merging_concat_axis1_join_axes.png)\n",
    "\n",
    "The join operation is made easy here as we have already defined the same indexes in both datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## \n",
    "\n",
    "merge() performs join operations similar to relational databases like SQL. Users who are familiar with SQL but new to pandas can reference a \n",
    "\n",
    "We use here the default operation which is \"left join\". THis is a type of join used to combine rows from two tables based on a related column between them. It returns all rows from the left table and includes matched rows from the right table. If there is no match, the result is `np.nan` for columns from the right table. Since we assigned indexes, where time and location_id define a unique observation, the join operations is based on this `pandas.MultIndex`. In other words, the left join ensures that all rows from the left table are included in the result set, even if there are no corresponding rows in the right table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide = df1.join(df2)\n",
    "df_wide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "This operation produced a “record” or “wide” format dataset, typically there is one row for each subject, in this case the indicators \"spei\" and \"zscores\" as well as the coordinates. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Simplifying Drought Severity with Data Binning\n",
    "\n",
    "We will convert the numeric data of the drought indicators `\"spei\"` and `\"zscore\"` into discrete categories using the pandas `cut` method. In pandas, binning data—also known as discretization or quantization—involves dividing continuous numerical data into discrete bins or intervals. This process is beneficial for simplifying data, managing outliers, creating histograms, and preparing data for machine learning algorithms that require categorical input. Additionally, we will label the binned data, thereby transforming the columns into pandas categorical data types. Pandas categorical data types are used to represent data that takes on a limited and usually fixed number of possible values (categories or classes). This type is particularly useful for categorical variables such as gender, days of the week, or survey responses. It offers efficient storage and operations for categorical data, including handling category ordering and missing values.\n",
    "\n",
    "The process of binning and labeling drought data based on intensity is somewhat subjective, as the bin thresholds are often arbitrarily assigned and subject to debate. Here, we adhere to the guidelines provided by the World Meteorological Organization and the definitions by McKee et al. (1993)[^1] for standardized Soil Moisture (SM) based drought indices, where a \"moderate\" drought is defined as starting at -1 unit of standard deviations.\n",
    "\n",
    "[^1]: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_labels = np.array([\"extreme\", \"severe\", \"moderate\", \"mild\", \"normal\"])\n",
    "zscore_thresholds = [df_wide[\"zscore\"].min(), -2, -1.5, -1, 0, df_wide[\"zscore\"].max()]\n",
    "spei_thresholds = [df_wide[\"spei\"].min(), -2, -1.5, -1, 0, df_wide[\"spei\"].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Now we can use the labels and thresholds to bind the columns of thew drouhgt indicators. We make a copy of the original data to preserve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide_cat = df_wide.copy()\n",
    "df_wide_cat[\"zscore\"] = pd.cut(df_wide.zscore, zscore_thresholds, labels=drought_labels)\n",
    "df_wide_cat[\"spei\"] = pd.cut(df_wide.spei, spei_thresholds, labels=drought_labels)\n",
    "df_wide_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "The simplified labelled drought indicators will now enables us a first step to assessing the spatial/areal extent.\n",
    "\n",
    "For a sanity check of our results we will recreate our plot from notebook 1 but now with categorical data types for the drought indicators. We however need to one thing before we can plot this data. We need to reshape the data in a \"long\" or \"stacked\" format for `hvplot`. We can do this with pandas `melt`, where we declare that we keep the indexes as well as the coordinates as they are, but the indicator values are stacked on top of each other, where the variable column indentifies the drought indictor.\n",
    "\n",
    "\n",
    "![Source: Ppandas User Guide](https://pandas.pydata.org/docs/user_guide)](https://pandas.pydata.org/docs/_images/reshaping_melt.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = df_wide_cat.melt(id_vars=[\"latitude\", \"longitude\"], ignore_index=False)\n",
    "df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long.hvplot.points(\n",
    "    x=\"longitude\",\n",
    "    y=\"latitude\",\n",
    "    c=\"value\",\n",
    "    groupby=[\"variable\", \"time\"],\n",
    "    x_sampling=0.1,\n",
    "    y_sampling=0.1,\n",
    "    aggregator=ds.by(\"value\", ds.any()),\n",
    "    rasterize=True,\n",
    "    crs=ccrs.PlateCarree(),\n",
    "    tiles=True,\n",
    "    frame_width=500,\n",
    "    clabel=\"Drought anomaly\",\n",
    "    cmap={\n",
    "        \"extreme\": \"#bb0c0c\",\n",
    "        \"severe\": \"#c57b19\",\n",
    "        \"moderate\": \"#b1bb29\",\n",
    "        \"mild\": \"#1cd87a\",\n",
    "        \"normal\": \"#ffffff\",\n",
    "    },\n",
    "    colorbar=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Spatial Extent\n",
    "\n",
    "Now that we simplified the drought intensity to classes, let's turn to calculating the spatial extend of droughts through time. This way we can see how both standardized drought indexes compare to each other. For this we can conveniently use the pandas `value_count` method on the new categorical columns `\"spei\"` and `\"zscore\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_spei = df_wide_cat.groupby(level=0)[\"spei\"].value_counts(normalize=True).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_zscore = (\n",
    "    df_wide_cat.groupby(level=0)[\"zscore\"].value_counts(normalize=True).unstack()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "We combine these results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_keys = pd.Index([\"spei\", \"zscore\"], name=\"indicator\")\n",
    "df_drought_extend = pd.concat(\n",
    "    [col_spei, col_zscore],\n",
    "    keys=new_keys,\n",
    ")\n",
    "df_drought_extend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "mozambique_droughts = [\n",
    "    {\"time\": \"2008-01-01\", \"people_affected\": 1.02},\n",
    "    {\"time\": \"2010-01-01\", \"people_affected\": 0.46},\n",
    "    {\"time\": \"2016-01-01\", \"people_affected\": 2.30},\n",
    "    {\"time\": \"2021-01-01\", \"people_affected\": 1.56},\n",
    "]\n",
    "\n",
    "df_droughts = pd.DataFrame(mozambique_droughts).assign(y=1)\n",
    "df_droughts[\"time\"] = pd.to_datetime(df_droughts[\"time\"], format=\"%Y-%M-%d\")\n",
    "df_droughts.set_index(\"time\", inplace=True)\n",
    "labels = df_droughts.hvplot.labels(\n",
    "    x=\"time\",\n",
    "    y=\"y\",\n",
    "    text=\"{people_affected} mill. people\",\n",
    "    text_baseline=\"bottom_left\",\n",
    "    hover=False,\n",
    "    angle=85,\n",
    "    text_font_size=\"14px\",\n",
    ")\n",
    "offset = hv.dim(\"y\") - 0.1\n",
    "points = df_droughts.hvplot.points(\n",
    "    x=\"time\", y=\"y\", color=\"black\", hover=False, transforms={\"y\": offset}\n",
    ")\n",
    "df_drought_extend.hvplot.area(\n",
    "    x=\"time\",\n",
    "    y=drought_labels[::-1][2:],\n",
    "    groupby=\"indicator\",\n",
    "    hover=False,\n",
    "    frame_width=800,\n",
    "    padding=((0.1, 0.1), (0, 0.9)),\n",
    ") * labels * points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "We see large differences for both drought indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = make_url(\"drought_indices-6_25_monthly.csv\")  # noqa\n",
    "df_drought_indices = pd.read_csv(\n",
    "    url,\n",
    "    index_col=[\"time\", \"location_id\"],\n",
    "    parse_dates=[\"time\"],\n",
    ")\n",
    "df_drought_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_drought_areal_extend(df):\n",
    "    # make drought categories\n",
    "    col_names = df.drop(columns=[\"longitude\", \"latitude\"]).columns\n",
    "    for name in col_names:\n",
    "        min_border = df[name].min()\n",
    "        max_border = df[name].max()\n",
    "        thresholds = np.array(\n",
    "            [\n",
    "                min_border if min_border < -2 else -2.1,\n",
    "                -2,\n",
    "                -1.5,\n",
    "                -1,\n",
    "                0,\n",
    "                max_border if max_border > 0 else 0.1,\n",
    "            ]\n",
    "        )\n",
    "        df[name] = pd.cut(df[name], thresholds, labels=drought_labels)\n",
    "\n",
    "    # calculate relative extend of drought\n",
    "    new_df = pd.concat(\n",
    "        [\n",
    "            df.groupby(level=0)[col].value_counts(normalize=True).unstack()\n",
    "            for col in col_names\n",
    "        ],\n",
    "        keys=pd.Index(col_names, name=\"indicator\"),\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "df_drought_extend = calc_drought_areal_extend(df_drought_indices.copy())\n",
    "df_drought_extend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drought_extend.hvplot.area(\n",
    "    x=\"time\",\n",
    "    y=drought_labels[::-1][2:],\n",
    "    groupby=\"indicator\",\n",
    "    hover=False,\n",
    "    frame_width=800,\n",
    "    padding=((0.1, 0.1), (0, 0.9)),\n",
    ") * labels * points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long.hvplot.points(\n",
    "    x=\"longitude\",\n",
    "    y=\"latitude\",\n",
    "    groupby=[\"variable\", \"time\"],\n",
    "    x_sampling=0.1,\n",
    "    y_sampling=0.1,\n",
    "    rasterize=True,\n",
    "    aggregator=ds.count_cat(\"value\"),\n",
    "    datashade=True,\n",
    "    crs=ccrs.PlateCarree(),\n",
    "    tiles=True,\n",
    "    frame_width=500,\n",
    "    clabel=\"Drought anomaly\",\n",
    "    cmap={\n",
    "        \"Extreme\": \"#bb0c0c\",\n",
    "        \"Severe\": \"#c57b19\",\n",
    "        \"moderate\": \"#b1bb29\",\n",
    "        \"mild\": \"#1cd87a\",\n",
    "        \"normal\": \"#ffffff\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confusion = pd.crosstab(df_wide_cat[\"spei\"], df_wide_cat[\"zscore\"], dropna=False)\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_drought = df_confusion.loc[\"drought\", :].sum()\n",
    "sensitivity = df_confusion.loc[\"drought\", \"drought\"] / tot_drought\n",
    "sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_no_drought = df_confusion.loc[\"no-drought\", :].sum()\n",
    "specificity = df_confusion.loc[\"no-drought\", \"no-drought\"] / tot_no_drought\n",
    "specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy = (sensitivity + specificity) / 2\n",
    "balanced_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environmental-remote-sensing",
   "language": "python",
   "name": "environmental-remote-sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
